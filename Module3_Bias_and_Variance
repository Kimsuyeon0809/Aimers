Module3 ML개론- 서울대학교 김건희 교수님 강의 공부 내용 (2)

1. Generalization
- 학습 data로 일반화 -> unseen data에 적용
- ML 모델의 목적 : unseen data에서 잘 동작하는 것

2. Generalization Error (Unseen data에 대한 error)
- Underfitting : Generalization error < Traning eror
- Overfitting : Generalization error > Traning eror
- 1차 목표 : Overfitting, 2차 목표 : 과도한 Overfitting인 경우 traning error 손해 보며 test error 낮추기

3. Typical relation between capacity and error
- capacity와 training error는 반비례하지만, 학습의 목적은 generalization(일반화) error의 반비례
- validation set 사용해 학습 중 generalization error 예측
- training error는 바로 측정이 가능하지만 generalization error는 바로 측정이 불가해 최적의 capacity를 찾는 것에 어려움이 있음 

4. Regularization
- 정규화를 통해 과적합 해결 
- 목적함수 J(w) = (error) + λw^tw 
- 목적함수 J(w) 최소화 = Loss 최소화 & Capacity 최소
- λ : hyper parameter / training error가 아닌 일반화 error 낮추려는 목적
- 같은 차수의 모델에서도 λ의 크기가 크면 underfitting 될 가능성이 높고, λ가 0에 가까우면 overfitting 될 가능성이 높다.
- λw^tw : 과적합 방지하는 regularization 항

5. Bias & Variance
- Bias : 예측의 평균과 true값의 차이
- Variance : 데이터 평균과 각 데이터의 거리 제곱의 평균 (E[(x-u)^2])
- Test error = Bias + Variance
- Bias와 Variance는 trade-off 관계 -> 앙상블 사용

6. Overfitting vs Underfitting
6-1) 높은 variance는 overfitting을 암시
- 데이터가 없는 부분이 널뛰지 않도록, train data가 많을수록 variance 감소
- Variance와 모델의 복잡도는 비례
6-2) 높은 bias는 underfitting을 암시
- bias가 높다는 것은, 영점이 맞지 않다는 것과 같아서, 정확하지 않은 모델을 의미한다. 때문에 underfitting과 연관되고, 복잡도가 너무 낮아진다.
- bias를 줄이려면 모델의 복잡도를 높여야 한다. 하지만, 그러면 variance도 높아진다.(Bias와 Variance는 trade-off 관계이기 때문)
